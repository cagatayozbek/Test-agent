import json
import glob
import os
import io
import re
import pandas as pd
from datetime import datetime

def parse_raw_logs(logs_path):
    """Parse raw_logs.jsonl to extract token usage and timing information."""
    if not os.path.exists(logs_path):
        return {
            "total_tokens": 0,
            "total_duration": 0,
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "agent_steps": 0
        }
    
    total_tokens = 0
    total_duration = 0
    prompt_tokens = 0
    completion_tokens = 0
    agent_steps = 0
    
    try:
        with open(logs_path, 'r') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    
                    # Count agent steps (assistant role = agent response)
                    if entry.get("role") == "assistant":
                        agent_steps += 1
                    
                    # Extract duration
                    if "duration_seconds" in entry:
                        total_duration += entry["duration_seconds"]
                    
                    # Extract token usage
                    if "token_usage" in entry:
                        tokens = entry["token_usage"]
                        total_tokens += tokens.get("total_tokens", 0)
                        prompt_tokens += tokens.get("prompt_tokens", 0)
                        completion_tokens += tokens.get("completion_tokens", 0)
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        print(f"Error parsing raw logs {logs_path}: {e}")
    
    return {
        "total_tokens": total_tokens,
        "total_duration": total_duration,
        "prompt_tokens": prompt_tokens,
        "completion_tokens": completion_tokens,
        "agent_steps": agent_steps
    }

def generate_report():
    results = []
    
    # Benchmark sonuÃ§larÄ±nÄ± benchmark_runs klasÃ¶rÃ¼nden oku
    base_dir = "benchmark_runs"
    
    if not os.path.exists(base_dir):
        print(f"âŒ {base_dir} klasÃ¶rÃ¼ bulunamadÄ±!")
        print("ğŸ’¡ Ã–nce './run_benchmark.sh' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return
    
    # GerÃ§ek task'larÄ± belirle (evaluation/tasks_v2/ klasÃ¶rÃ¼ndeki task'lar)
    real_tasks = set()
    tasks_dir = "evaluation/tasks_v2"
    if os.path.exists(tasks_dir):
        real_tasks = set(os.listdir(tasks_dir))
        # Gizli dosyalarÄ± ve __pycache__ gibi dizinleri Ã§Ä±kar
        real_tasks = {t for t in real_tasks if not t.startswith('.') and t != '__pycache__'}
        print(f"ğŸ“‹ GerÃ§ek task sayÄ±sÄ±: {len(real_tasks)}")
        print(f"âœ… GerÃ§ek task'lar: {sorted(real_tasks)}\n")
    else:
        print(f"âš ï¸  {tasks_dir} bulunamadÄ±, tÃ¼m task'lar dahil edilecek.\n")
    
    # TÃ¼m run klasÃ¶rlerini gez
    for task_name in os.listdir(base_dir):
        task_path = os.path.join(base_dir, task_name)
        
        # GerÃ§ek task kontrolÃ¼
        if real_tasks and task_name not in real_tasks:
            print(f"â­ï¸  AtlanÄ±yor (gerÃ§ek task deÄŸil): {task_name}")
            continue
        if not os.path.isdir(task_path):
            continue
            
        for run_id in os.listdir(task_path):
            run_path = os.path.join(task_path, run_id)
            summary_path = os.path.join(run_path, "summary.json")
            raw_logs_path = os.path.join(run_path, "raw_logs.jsonl")
            
            if not os.path.exists(summary_path):
                continue
                
            try:
                with open(summary_path, 'r') as f:
                    data = json.load(f)
                
                # Extract Mode from run_id (directory name)
                mode = "unknown"
                if run_id.startswith("agentic"):
                    mode = "agentic"
                elif run_id.startswith("baseline"):
                    mode = "baseline"
                
                # Extract Success and Attempts from hypothesis text
                hypothesis_text = data.get("hypothesis", {}).get("hypothesis", "")
                success = False
                attempts = 0
                
                if "succeeded" in hypothesis_text.lower():
                    success = True
                
                match = re.search(r"after (\d+) attempts", hypothesis_text)
                if match:
                    attempts = int(match.group(1))
                
                # Parse raw logs for detailed metrics
                log_metrics = parse_raw_logs(raw_logs_path)

                # Veriyi dÃ¼zleÅŸtir
                row = {
                    "Task": task_name,
                    "Mode": mode,
                    "Model": data.get("model_id", "unknown"),
                    "Run ID": run_id,
                    "Success": success,
                    "Attempts": attempts,
                    "Tool Calls": data.get("tool_call_count", 0),
                    "Total Tokens": log_metrics["total_tokens"],
                    "Prompt Tokens": log_metrics["prompt_tokens"],
                    "Completion Tokens": log_metrics["completion_tokens"],
                    "Duration (s)": round(log_metrics["total_duration"], 2),
                    "Agent Steps": log_metrics["agent_steps"]
                }
                results.append(row)
            except Exception as e:
                print(f"Error reading {summary_path}: {e}")

    if not results:
        print("No results found.")
        return

    df = pd.read_json(io.StringIO(json.dumps(results)))
    
    # Rapor Markdown
    md = "# Benchmark Raporu: Baseline vs Agentic (DetaylÄ± Analiz)\n\n"
    md += f"**Rapor Tarihi:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    # 1. Genel Ã–zet (Model + Mode bazÄ±nda)
    md += "## 1. Genel Ã–zet (Model ve Mode BazÄ±nda)\n\n"
    summary = df.groupby(['Model', 'Mode']).agg({
        'Success': 'mean',  # BaÅŸarÄ± oranÄ±
        'Attempts': 'mean',
        'Tool Calls': 'mean',
        'Total Tokens': 'mean',
        'Duration (s)': 'mean',
        'Agent Steps': 'mean',
        'Run ID': 'count'  # KaÃ§ run var
    }).reset_index()
    
    summary.columns = ['Model', 'Mode', 'Success Rate (%)', 'Avg Attempts', 
                       'Avg Tool Calls', 'Avg Total Tokens', 'Avg Duration (s)', 
                       'Avg Agent Steps', 'Run Count']
    summary['Success Rate (%)'] = (summary['Success Rate (%)'] * 100).round(1)
    
    md += summary.to_markdown(index=False, floatfmt=".1f")
    md += "\n\n"
    
    # 2. Task BazlÄ± BaÅŸarÄ± OranÄ±
    md += "## 2. Task BazlÄ± BaÅŸarÄ± Analizi\n\n"
    task_success = df.pivot_table(
        index=['Task'], 
        columns=['Model', 'Mode'], 
        values='Success',
        aggfunc='mean'  # Ortalama baÅŸarÄ± oranÄ±
    ) * 100  # YÃ¼zdeye Ã§evir
    md += task_success.to_markdown(floatfmt=".0f")
    md += "\n\n"
    
    # 3. Task BazlÄ± Ortalama Attempt SayÄ±sÄ±
    md += "## 3. Task BazlÄ± Ortalama Attempt SayÄ±sÄ±\n\n"
    task_attempts = df.pivot_table(
        index=['Task'], 
        columns=['Model', 'Mode'], 
        values='Attempts',
        aggfunc='mean'
    )
    md += task_attempts.to_markdown(floatfmt=".1f")
    md += "\n\n"
    
    # 4. Task BazlÄ± Ortalama Token KullanÄ±mÄ±
    md += "## 4. Task BazlÄ± Ortalama Token KullanÄ±mÄ±\n\n"
    task_tokens = df.pivot_table(
        index=['Task'], 
        columns=['Model', 'Mode'], 
        values='Total Tokens',
        aggfunc='mean'
    )
    md += task_tokens.to_markdown(floatfmt=".0f")
    md += "\n\n"
    
    # 5. Task BazlÄ± Ortalama SÃ¼re
    md += "## 5. Task BazlÄ± Ortalama SÃ¼re (saniye)\n\n"
    task_duration = df.pivot_table(
        index=['Task'], 
        columns=['Model', 'Mode'], 
        values='Duration (s)',
        aggfunc='mean'
    )
    md += task_duration.to_markdown(floatfmt=".1f")
    md += "\n\n"
    
    # 6. DetaylÄ± Run Listesi (baÅŸarÄ±sÄ±z olanlar)
    md += "## 6. BaÅŸarÄ±sÄ±z Runlar (Detay)\n\n"
    failed_runs = df[df['Success'] == False][['Task', 'Model', 'Mode', 'Attempts', 
                                               'Total Tokens', 'Duration (s)', 'Run ID']]
    if not failed_runs.empty:
        md += failed_runs.to_markdown(index=False, floatfmt=".1f")
    else:
        md += "TÃ¼m runlar baÅŸarÄ±lÄ±! ğŸ‰\n"
    md += "\n\n"
    
    # 7. Maliyet Analizi (Token bazlÄ± tahmini)
    md += "## 7. Maliyet Analizi (Tahmini)\n\n"
    md += "**Not:** Maliyet hesaplamalarÄ± Gemini pricing'e gÃ¶re yaklaÅŸÄ±k deÄŸerlerdir.\n\n"
    
    # Gemini 2.0 Flash: $0.30/1M input, $1.20/1M output (128k context)
    # Gemini 2.5 Flash: $0.30/1M input, $1.20/1M output (128k context)  
    # Gemini 2.5 Pro: $3.50/1M input, $10.50/1M output (128k context)
    
    cost_data = []
    for _, row in summary.iterrows():
        model = row['Model']
        mode = row['Mode']
        avg_prompt = df[(df['Model'] == model) & (df['Mode'] == mode)]['Prompt Tokens'].mean()
        avg_completion = df[(df['Model'] == model) & (df['Mode'] == mode)]['Completion Tokens'].mean()
        
        if 'flash' in model.lower():
            input_cost = (avg_prompt / 1_000_000) * 0.30
            output_cost = (avg_completion / 1_000_000) * 1.20
        elif 'pro' in model.lower():
            input_cost = (avg_prompt / 1_000_000) * 3.50
            output_cost = (avg_completion / 1_000_000) * 10.50
        else:
            input_cost = 0
            output_cost = 0
        
        total_cost = input_cost + output_cost
        
        cost_data.append({
            'Model': model,
            'Mode': mode,
            'Avg Prompt Tokens': avg_prompt,
            'Avg Completion Tokens': avg_completion,
            'Cost per Run ($)': total_cost,
            'Run Count': row['Run Count'],
            'Total Cost ($)': total_cost * row['Run Count']
        })
    
    cost_df = pd.DataFrame(cost_data)
    md += cost_df.to_markdown(index=False, floatfmt=".4f")
    md += "\n\n"
    
    with open("benchmark_report.md", "w") as f:
        f.write(md)
    
    print("âœ… DetaylÄ± rapor oluÅŸturuldu: benchmark_report.md")

if __name__ == "__main__":
    generate_report()
