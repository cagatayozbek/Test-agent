# Test Generation Pipeline (A8) - Uygulama Raporu

**Tarih:** 1 Ocak 2026  
**Milestone:** A8 - Test Generation Pipeline  
**Durum:** âœ… TamamlandÄ±

---

## ğŸ“‹ Ã–zet

Bug detection odaklÄ± mevcut sisteme **test generation** kapasitesi eklendi. Yeni sistem, LLM'in bug-revealing testler yazmasÄ±nÄ±, bu testleri hem buggy hem fixed kod Ã¼zerinde doÄŸrulamasÄ±nÄ± ve sonuÃ§larÄ± BRTR (Bug-Revealing Test Rate) metriÄŸi ile deÄŸerlendirmesini saÄŸlÄ±yor.

---

## ğŸ”§ YapÄ±lan DeÄŸiÅŸiklikler

### 1. Agent Mimarisi GeniÅŸletmesi

#### `agents/agent_graph.yaml`

```yaml
# Ã–nceki agentic akÄ±ÅŸ:
# planner â†’ analysis â†’ critic â†’ reflection â†’ executor

# Yeni agentic akÄ±ÅŸ:
planner â†’ analysis â†’ testwriter â†’ critic â†’ reflection â†’ executor
# Baseline modu deÄŸiÅŸikliÄŸi:
# Ã–nceki: executor (tek agent)
# Yeni: testwriter (tek agent - sadece test generation)
```

#### `prompts/testwriter.txt` (YENÄ°)

- pytest odaklÄ± bug-revealing test prompt'u
- JSON output formatÄ±: `tool`, `args`, `test_metadata`
- Few-shot Ã¶rnekler: boundary bug, cache invalidation bug
- Kritik gereksinimler: "Test MUST FAIL on buggy, MUST PASS on fixed"

---

### 2. Test DosyasÄ± YÃ¶netimi

#### `tools/__init__.py`

```python
# Yeni fonksiyon eklendi:
def write_test_file(
    output_dir: Path,
    filename: str,
    content: str,
    attempt: int = 1
) -> dict[str, Any]:
    """Write a generated test file to the output directory."""
```

**Ã–zellikler:**

- Otomatik dizin oluÅŸturma (`mkdir -p` davranÄ±ÅŸÄ±)
- Attempt-based isolation: `test_generated_attempt_2.py`
- UTF-8 encoding
- Soft error handling (exception yerine dict dÃ¶nÃ¼ÅŸÃ¼)

#### `instrumented_tools.py`

- `write_test_file()` wrapper eklendi
- String-to-Path dÃ¶nÃ¼ÅŸÃ¼mÃ¼
- Counter increment entegrasyonu

#### `run_paths.py`

```python
@dataclass
class RunPaths:
    root: Path
    raw_logs: Path
    summary: Path
    tool_outputs: Path
    generated_tests: Path  # YENÄ°
```

#### `custom_session.py`

- `tool_map`'e `write_test_file` eklendi
- `_write_test_file_in_run_dir()` metodu: Test dosyalarÄ±nÄ± run dizinine yazar

#### `prompts/executor.txt`

- `write_test_file` tool dokÃ¼mantasyonu eklendi
- Few-shot Ã¶rnek eklendi

---

### 3. Bug-Revealing Test DoÄŸrulama DÃ¶ngÃ¼sÃ¼

#### `task_loader.py`

**Yeni class: `TaskContextV2`**

```python
@dataclass
class TaskContextV2:
    task_id: str
    buggy_code: str      # Agent'a gÃ¶sterilir
    fixed_code: str      # Sadece validation iÃ§in
    metadata: dict
    buggy_path: Path
    fixed_path: Path
```

**Yeni fonksiyon: `run_test_on_both_versions()`**

```python
def run_test_on_both_versions(
    test_file_path: Path,
    buggy_dir: Path,
    fixed_dir: Path,
    timeout: int = 60
) -> dict:
    """
    Returns:
        buggy_failed: bool
        fixed_passed: bool
        is_bug_revealing: buggy_failed AND fixed_passed
    """
```

#### `config.yaml`

```yaml
# Yeni ayarlar:
test_generation:
  max_retry_attempts: 3
  test_timeout_seconds: 60
```

---

### 4. Schema GeniÅŸletmesi

#### `schemas/models.py`

**Yeni model: `TestGenerationResult`**

```python
class TestGenerationResult(BaseModel):
    attempt: int
    test_file: str
    buggy_failed: bool
    fixed_passed: bool
    is_bug_revealing: bool
    buggy_output: str = ""
    fixed_output: str = ""
```

**Yeni model: `TestGenerationSummary`**

```python
class TestGenerationSummary(BaseModel):
    # Base fields
    hypothesis: SemanticHypothesis | None
    evaluation: EvaluationResult | None
    model_id: str
    timestamp: str
    tool_call_count: int

    # Test generation fields
    task_id: str
    mode: Literal["baseline", "agentic"]
    tests_generated: int
    attempts_until_success: int | None
    buggy_failed: bool
    fixed_passed: bool
    is_bug_revealing: bool
    overfitting_detected: bool
    test_results: list[TestGenerationResult]

    def calculate_brtr(self) -> float:
        """Bug-Revealing Test Rate: 1.0 if success, 0.0 otherwise"""
```

#### `schemas/__init__.py`

- `TestGenerationResult` ve `TestGenerationSummary` export'larÄ± eklendi

---

### 5. Task YapÄ±sÄ± v2

#### Yeni dizin yapÄ±sÄ±: `evaluation/tasks_v2/`

```
evaluation/tasks_v2/
â”œâ”€â”€ boundary_threshold/
â”‚   â”œâ”€â”€ buggy/
â”‚   â”‚   â””â”€â”€ source.py      # > yerine >= kullanÄ±lmalÄ±
â”‚   â”œâ”€â”€ fixed/
â”‚   â”‚   â””â”€â”€ source.py      # >= doÄŸru kullanÄ±m
â”‚   â””â”€â”€ metadata.json
â””â”€â”€ cache_invalidation/
    â”œâ”€â”€ buggy/
    â”‚   â””â”€â”€ source.py      # logout() cache temizlemiyor
    â”œâ”€â”€ fixed/
    â”‚   â””â”€â”€ source.py      # logout() cache.clear() Ã§aÄŸÄ±rÄ±yor
    â””â”€â”€ metadata.json
```

#### Ã–rnek `metadata.json` formatÄ±:

```json
{
  "task_id": "boundary_threshold",
  "title": "VIP Threshold Boundary Bug",
  "bug_type": "boundary_condition",
  "bug_description": "Uses > instead of >= for threshold check",
  "bug_location": {
    "file": "source.py",
    "function": "calculate_discount",
    "line": 35
  },
  "expected_failure_signal": "AssertionError on boundary value",
  "test_hint": "Test with exactly LOYALTY_THRESHOLD points"
}
```

---

### 6. Evaluation GÃ¼ncellemesi

#### `evaluation/run_all.py`

**Yeni fonksiyon: `discover_tasks_v2()`**

```python
def discover_tasks_v2(tasks_dir: Path) -> list[str]:
    """evaluation/tasks_v2/ altÄ±ndaki test generation task'larÄ±nÄ± bul."""
```

**Yeni fonksiyon: `run_test_generation_tasks()`**

```python
def run_test_generation_tasks(...) -> dict:
    """
    1. Agent'Ä± Ã§alÄ±ÅŸtÄ±r (test Ã¼ret)
    2. Generated test'i bul
    3. Buggy ve fixed Ã¼zerinde Ã§alÄ±ÅŸtÄ±r
    4. BRTR hesapla
    """
```

**Yeni CLI flag:**

```bash
python evaluation/run_all.py --test-gen --mode both
```

**Output formatÄ±:**

```json
{
  "evaluation_type": "test_generation",
  "results": [...],
  "brtr_summary": {
    "baseline": 0.5,
    "agentic": 0.8
  }
}
```

---

### 7. Failure Analysis

#### `evaluation/failure_analyzer.py` (YENÄ°)

**FailureCategory enum:**
| Kategori | AÃ§Ä±klama |
|----------|----------|
| `success` | Bug-revealing test (buggy fail, fixed pass) |
| `no_fail` | Test buggy kod'da pass ediyor |
| `overfit` | Test her iki versiyonda da fail |
| `flaky` | Non-deterministic sonuÃ§lar |
| `wrong_assert` | Assertion yanlÄ±ÅŸ davranÄ±ÅŸÄ± hedefliyor |
| `wrong_input` | Input bug'Ä± tetiklemiyor |
| `wrong_state` | Setup bug durumunu oluÅŸturmuyor |

**Ana fonksiyonlar:**

```python
def classify_failure(buggy_failed, fixed_passed, ...) -> FailureCategory
def analyze_test_code(test_content: str) -> Optional[FailureCategory]
def analyze_pytest_output(output: str) -> dict
```

**FailureAnalyzer class:**

```python
analyzer = FailureAnalyzer()
analyzer.add_record(task_id, attempt, test_file, ...)
analyzer.get_summary()  # {"success": 3, "no_fail": 2, ...}
analyzer.save_analysis(Path("failures.json"))
analyzer.print_summary()
```

---

## ğŸ“Š DeÄŸiÅŸiklik Ã–zeti

| Dosya                                     | DeÄŸiÅŸiklik Tipi | SatÄ±r DeÄŸiÅŸikliÄŸi |
| ----------------------------------------- | --------------- | ----------------- |
| `agents/agent_graph.yaml`                 | GÃ¼ncelleme      | +3 satÄ±r          |
| `prompts/testwriter.txt`                  | Yeni dosya      | ~100 satÄ±r        |
| `prompts/executor.txt`                    | GÃ¼ncelleme      | +5 satÄ±r          |
| `tools/__init__.py`                       | GÃ¼ncelleme      | +70 satÄ±r         |
| `instrumented_tools.py`                   | GÃ¼ncelleme      | +30 satÄ±r         |
| `run_paths.py`                            | GÃ¼ncelleme      | +2 satÄ±r          |
| `custom_session.py`                       | GÃ¼ncelleme      | +225 satÄ±r        |
| `config.py`                               | GÃ¼ncelleme      | +15 satÄ±r         |
| `config.yaml`                             | GÃ¼ncelleme      | +4 satÄ±r          |
| `task_loader.py`                          | GÃ¼ncelleme      | +150 satÄ±r        |
| `schemas/models.py`                       | GÃ¼ncelleme      | +80 satÄ±r         |
| `schemas/__init__.py`                     | GÃ¼ncelleme      | +10 satÄ±r         |
| `evaluation/run_all.py`                   | GÃ¼ncelleme      | +270 satÄ±r        |
| `evaluation/test_evaluator.py`            | Yeni dosya      | ~430 satÄ±r        |
| `evaluation/failure_analyzer.py`          | Yeni dosya      | ~300 satÄ±r        |
| `evaluation/tasks_v2/boundary_threshold/` | Yeni klasÃ¶r     | 3 dosya           |
| `evaluation/tasks_v2/cache_invalidation/` | Yeni klasÃ¶r     | 3 dosya           |

**Toplam:** ~18 dosya, ~1700+ satÄ±r yeni/deÄŸiÅŸtirilmiÅŸ kod

---

## ğŸš€ KullanÄ±m

### Test Generation Modunu Ã‡alÄ±ÅŸtÄ±rma

```bash
# TÃ¼m task'larÄ± her iki modda Ã§alÄ±ÅŸtÄ±r
python evaluation/run_all.py --test-gen --mode both

# Tek task
python evaluation/run_all.py --test-gen --task boundary_threshold --mode agentic

# Verbose output
python evaluation/run_all.py --test-gen --mode both -v
```

### Beklenen Output

```
ğŸ§ª Test Generation Mode
Discovered 2 task(s): ['boundary_threshold', 'cache_invalidation']
Modes: ['baseline', 'agentic']
--------------------------------------------------
[boundary_threshold] mode=baseline run_id=baseline_20260101_120000
  ğŸ¯ BRTR: buggy_fail=True, fixed_pass=True â†’ bug_revealing=True
  âœ“ OK
[boundary_threshold] mode=agentic run_id=agentic_20260101_120030
  ğŸ¯ BRTR: buggy_fail=True, fixed_pass=True â†’ bug_revealing=True
  âœ“ OK
--------------------------------------------------
Total: 4 | Passed: 4 | Bug-Revealing: 3

ğŸ“Š BRTR Summary:
  baseline: 50.0%
  agentic: 100.0%
```

---

## ğŸ”® Sonraki AdÄ±mlar

1. **Paper HazÄ±rlÄ±ÄŸÄ± (A9):** Threats to validity, experimental setup, key findings
2. **Daha fazla task:** FarklÄ± bug tÃ¼rleri iÃ§in v2 task'lar ekle

### âœ… Tamamlanan Sonraki AdÄ±mlar

3. ~~**Retry mekanizmasÄ±:** BaÅŸarÄ±sÄ±z test generation'da otomatik retry~~ âœ… Eklendi (1 Ocak 2026)
4. ~~**LLM-based failure analysis:** Neden bug-revealing olmadÄ±?~~ âœ… Eklendi

---

## ğŸ†• GÃ¼ncelleme: Execution-Based BRTR + LLM Analysis (1 Ocak 2026 - AkÅŸam)

### Mimari KararÄ±

**BRTR (Bug-Revealing Test Rate)** metriÄŸi **execution-based** olmalÄ±:

```
is_bug_revealing = buggy_failed AND fixed_passed
                   ^^^^^^^^^^^^     ^^^^^^^^^^^^
                   pytest exit      pytest exit
                   code != 0        code == 0
```

**Neden?**

- Deterministic ve reproducible
- Reviewer-friendly (execution truth)
- LLM hallucination riski yok
- Paper'da savunmasÄ± kolay

### LLM'in RolÃ¼: AÃ§Ä±klayÄ±cÄ±, Karar Verici DeÄŸil

LLM ÅŸimdi **analiz ve aÃ§Ä±klama** saÄŸlÄ±yor, BRTR'Ä± **etkilemiyor**:

| Metric             | Source                          | LLM Etkisi       |
| ------------------ | ------------------------------- | ---------------- |
| `is_bug_revealing` | pytest exit codes               | âŒ Yok           |
| `BRTR`             | count(is_bug_revealing) / total | âŒ Yok           |
| `failure_category` | LLM analysis                    | âœ“ AÃ§Ä±klayÄ±cÄ±     |
| `retry_suggestion` | LLM analysis                    | âœ“ Ã–ÄŸretici       |
| `commentary`       | LLM analysis                    | âœ“ Bilgilendirici |

### Yeni Mimari: TestEvaluator (AÃ§Ä±klayÄ±cÄ± Rol)

#### `evaluation/test_evaluator.py`

```python
class TestEvaluator:
    """LLM-based analyzer for test results (not judge)."""

    def evaluate_test(...) -> TestEvaluationResult:
        """
        Provides analysis and explanation.
        Does NOT determine is_bug_revealing - that's execution-based.
        """
```

**LLM'in saÄŸladÄ±ÄŸÄ± analiz:**

- `llm_verdict`: LLM'in kendi deÄŸerlendirmesi (karÅŸÄ±laÅŸtÄ±rma iÃ§in)
- `confidence`: high/medium/low
- `failure_category`: success, no_fail, overfit, wrong_assert, wrong_input, etc.
- `buggy_analysis`: Buggy kod Ã¼zerinde ne oldu
- `fixed_analysis`: Fixed kod Ã¼zerinde ne oldu
- `why_not_revealing`: Neden bug-revealing deÄŸil (eÄŸer deÄŸilse)
- `retry_suggestion`: Bir sonraki deneme iÃ§in Ã¶neri
- `test_quality_score`: 1-10 arasÄ± kalite puanÄ±

#### `evaluation/run_all.py`

```python
def run_test_generation_tasks(...):
    # BRTR is execution-based (deterministic)
    is_bug_revealing = validation["buggy_failed"] and validation["fixed_passed"]

    # LLM provides analysis (not the verdict)
    eval_result = test_evaluator.evaluate_test(...)

    result["test_validation"] = {
        "is_bug_revealing": is_bug_revealing,  # Execution-based
        "llm_analysis": {
            "llm_verdict": eval_result.is_bug_revealing,  # For comparison
            "failure_category": eval_result.failure_category,
            "commentary": eval_result.commentary,
            ...
        }
    }
```

### Output FormatÄ±

```
[boundary_threshold] mode=agentic run_id=agentic_20260101_120030
  ğŸ¯ Execution: buggy_fail=True, fixed_pass=True â†’ bug_revealing=True
     ğŸ¤– LLM Analysis: success [high] - Test correctly targets the boundary...
  âœ“ OK
```

### Ã–rnek LLM Analysis Output

```json
{
  "llm_verdict": true,
  "confidence": "high",
  "failure_category": "success",
  "buggy_analysis": "Test failed with AssertionError: expected 20% discount but got 0%",
  "fixed_analysis": "Test passed, discount correctly applied at boundary",
  "why_not_revealing": "",
  "retry_suggestion": "",
  "test_quality_score": 9,
  "commentary": "Excellent test - directly targets the >= vs > boundary condition"
}
```

### Retry Feedback MekanizmasÄ±

LLM analizi, baÅŸarÄ±sÄ±z denemelerde **Ã¶ÄŸretici feedback** saÄŸlÄ±yor:

```python
# custom_session.py
class CustomSession:
    def __init__(self, ..., retry_context: str = ""):
        self.retry_context = retry_context  # LLM'den gelen Ã¶neri
```

**AkÄ±ÅŸ:**

1. Test execution-based olarak `is_bug_revealing=False` â†’ baÅŸarÄ±sÄ±z
2. LLM analiz eder: "wrong_input - value=50 kullandÄ±n ama bug value=100'de"
3. Bu feedback `retry_context` olarak sonraki denemeye geÃ§irilir
4. TestWriter agent bu context'i gÃ¶rÃ¼r, aynÄ± hatayÄ± tekrarlamaz

### Paper Ä°Ã§in Avantajlar

| Ã–zellik         | Eski (LLM-judge) | Yeni (Execution-based) |
| --------------- | ---------------- | ---------------------- |
| Reproducibility | âŒ LLM'e baÄŸlÄ±   | âœ“ Deterministic        |
| Reviewer gÃ¼veni | "LLM judge?"     | "Execution truth âœ“"    |
| Methodology     | Questionable     | Standard               |
| LLM cost        | Her test iÃ§in    | Ä°steÄŸe baÄŸlÄ± analiz    |

---

## ğŸ†• GÃ¼ncelleme: Retry MekanizmasÄ± (1 Ocak 2026 - Gece)

### Motivasyon

Ä°lk test generation denemesi her zaman bug-revealing olmayabilir. LLM'in yanlÄ±ÅŸ input, yanlÄ±ÅŸ assertion veya eksik setup kullanmasÄ± muhtemel. Retry mekanizmasÄ±, baÅŸarÄ±sÄ±z denemelerdeki feedback'i kullanarak iteratif iyileÅŸtirme saÄŸlÄ±yor.

### Yeni Class'lar

#### `config.py`

```python
class TestGenerationConfig(BaseModel):
    """Configuration for test generation mode."""
    max_retry_attempts: int = 3
    test_timeout_seconds: int = 60


class Config(BaseModel):
    model_id: str
    max_turns: int
    timeout_seconds: int
    test_generation: Optional[TestGenerationConfig] = None
```

#### `custom_session.py`

**Yeni dataclass: `TestGenerationResult`**

```python
@dataclass
class TestGenerationResult:
    """Result from a single test generation attempt."""
    attempt: int
    test_file: Path | None
    test_code: str
    is_bug_revealing: bool
    buggy_failed: bool
    fixed_passed: bool
    evaluation: TestEvaluationResult | None = None
```

**Yeni dataclass: `TestGenerationSessionResult`**

```python
@dataclass
class TestGenerationSessionResult:
    """Final result from test generation session with retries."""
    success: bool
    attempts: int
    results: list[TestGenerationResult]
    final_test_file: Path | None = None
```

**Yeni class: `TestGenerationSession`**

```python
class TestGenerationSession:
    """Session for test generation with retry logic.

    Manages the retry loop:
    1. Run agent pipeline to generate test
    2. Validate test against buggy/fixed code
    3. If not bug-revealing, get feedback from TestEvaluator
    4. Inject feedback as retry_context for next attempt
    5. Run again with improved context
    6. Repeat until success or max retries
    """

    def __init__(
        self,
        graph: AgentGraph,
        mode: str,
        prompts: dict[str, str],
        tools: InstrumentedTools,
        log_path: Path,
        llm: GeminiClient,
        task_context: TaskContextV2,
        test_evaluator: TestEvaluator,
        max_retries: int = 3,
        test_timeout: int = 60,
    ) -> None: ...

    def run(self) -> TestGenerationSessionResult:
        """Run test generation with retry loop."""
```

### Retry AkÄ±ÅŸÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Attempt 1                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Run CustomSession (plannerâ†’analysisâ†’testwriterâ†’...)     â”‚
â”‚  2. Find generated test file                                 â”‚
â”‚  3. Validate: run_test_on_both_versions()                   â”‚
â”‚  4. is_bug_revealing? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º SUCCESS âœ…  â”‚
â”‚         â”‚ No                                                 â”‚
â”‚         â–¼                                                    â”‚
â”‚  5. TestEvaluator.evaluate_test()                           â”‚
â”‚  6. Get retry_context with failure analysis                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Attempt 2                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. CustomSession with retry_context injected               â”‚
â”‚     (ConversationHistory.set_retry_context())               â”‚
â”‚  2. TestWriter sees previous failure + suggestion           â”‚
â”‚  3. Generate improved test                                   â”‚
â”‚  4. Validate again...                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                    (max_retries'a kadar)
```

### Context Injection

Retry context, `ConversationHistory` Ã¼zerinden TestWriter agent'a geÃ§iriliyor:

```python
# ConversationHistory.get_context_for_agent()
def get_context_for_agent(self, current_agent: str) -> str:
    context_parts = []

    # Include retry context for testwriter agent
    if current_agent == "testwriter" and self.retry_context:
        context_parts.append(self.retry_context)
        context_parts.append("")

    # ... rest of context building
```

### Retry Context FormatÄ±

```
=== PREVIOUS TEST GENERATION ATTEMPTS ===
Total attempts so far: 2

### Attempt 1
- Result: no_fail
- Bug-Revealing: âœ—
- Analysis: Test passed on buggy code - input value=50 doesn't trigger the bug
- Suggestion: Use exactly 100 points (LOYALTY_THRESHOLD) to hit the boundary

### Attempt 2
- Result: wrong_assert
- Bug-Revealing: âœ—
- Analysis: Test asserts wrong expected value
- Suggestion: Expected discount is 20%, not 10%

=== USE THIS FEEDBACK TO IMPROVE YOUR TEST ===
```

### CLI GÃ¼ncellemesi

```bash
# VarsayÄ±lan 3 retry ile
python evaluation/run_all.py --test-gen --mode both

# Ã–zel retry sayÄ±sÄ± ile
python evaluation/run_all.py --test-gen --mode agentic --max-retries 5
```

### Output FormatÄ±

```
ğŸ§ª Test Generation Mode (with retry)
Discovered 2 task(s): ['boundary_threshold', 'cache_invalidation']
Modes: ['baseline', 'agentic']
Max retries: 3
--------------------------------------------------
âœ“ Config loaded: max_retries=3, timeout=60s

============================================================
[boundary_threshold] mode=agentic run_id=agentic_20260101_180000
Bug: Uses > instead of >= for threshold check...
============================================================

============================================================
ğŸ§ª TEST GENERATION ATTEMPT 1/3
============================================================
ğŸ¤– [PLANNER] calling LLM...
...
ğŸ“„ Generated test: test_generated.py
ğŸ”¬ Validating test...
âŒ Validation: buggy_fail=False, fixed_pass=True â†’ bug_revealing=False
ğŸ”„ Getting feedback for retry...
   Category: no_fail
   Suggestion: Use boundary value 100 instead of 150...

============================================================
ğŸ§ª TEST GENERATION ATTEMPT 2/3
============================================================
ğŸ“‹ Retry context loaded from previous attempts
ğŸ¤– [PLANNER] calling LLM...
...
ğŸ“„ Generated test: test_generated.py
ğŸ”¬ Validating test...
ğŸ¯ Validation: buggy_fail=True, fixed_pass=True â†’ bug_revealing=True
âœ… SUCCESS! Bug-revealing test generated on attempt 2

ğŸ¯ Final: bug_revealing=True (attempts: 2)

--------------------------------------------------
Total: 4 | Passed: 4 | Bug-Revealing: 4

ğŸ“Š BRTR Summary:
  baseline: 50.0% (avg attempts: 2.5)
  agentic: 100.0% (avg attempts: 1.5)
```

### Yeni Report AlanlarÄ±

```json
{
  "evaluation_type": "test_generation",
  "max_retries": 3,
  "results": [
    {
      "task_id": "boundary_threshold",
      "mode": "agentic",
      "success": true,
      "attempts": 2,
      "test_validation": {
        "is_bug_revealing": true,
        "test_file": "runs/.../generated_tests/test_generated.py",
        "attempts_detail": [
          {
            "attempt": 1,
            "buggy_failed": false,
            "fixed_passed": true,
            "is_bug_revealing": false,
            "failure_category": "no_fail"
          },
          {
            "attempt": 2,
            "buggy_failed": true,
            "fixed_passed": true,
            "is_bug_revealing": true,
            "failure_category": "success"
          }
        ]
      }
    }
  ],
  "brtr_summary": {
    "baseline": 0.5,
    "agentic": 1.0
  },
  "attempts_stats": {
    "baseline": {
      "avg_attempts_to_success": 2.5,
      "success_count": 1
    },
    "agentic": {
      "avg_attempts_to_success": 1.5,
      "success_count": 2
    }
  }
}
```

### DeÄŸiÅŸiklik Ã–zeti (Retry)

| Dosya                   | DeÄŸiÅŸiklik Tipi | SatÄ±r DeÄŸiÅŸikliÄŸi |
| ----------------------- | --------------- | ----------------- |
| `config.py`             | GÃ¼ncelleme      | +15 satÄ±r         |
| `custom_session.py`     | GÃ¼ncelleme      | +200 satÄ±r        |
| `evaluation/run_all.py` | GÃ¼ncelleme      | +150 satÄ±r        |

**Toplam:** ~365 satÄ±r yeni kod

---

_Rapor gÃ¼ncelleme tarihi: 1 Ocak 2026 (Gece - Retry MekanizmasÄ±)_
